# 3.1，特征缩放(Feature Scaling)

> 因不同参数都不在一个数量级上，这样会导致θi的数值也不能在一个数量级，最终导致不能够简单快速的收敛。
>
> 因此，提出了特征缩放来解决这个问题。

> **特征缩放**：将所有的特征变量压缩到大致为<img  src="http://latex.codecogs.com/svg.latex?\left&space;[&space;-1,1\right&space;]"  title="http://latex.codecogs.com/svg.latex?\left [ -1,1\right ]" />的范围。

>比如：
>          <img src="http://latex.codecogs.com/svg.latex?x_{i}=\frac{x_{i}}{s_{i}}" title="http://latex.codecogs.com/svg.latex?x_{i}=\frac{x_{i}}{s_{i}}" />其中，<img src="http://latex.codecogs.com/svg.latex?s_{i}" title="http://latex.codecogs.com/svg.latex?s_{i}" />是最大值-最小值，这样就缩放到<img  src="http://latex.codecogs.com/svg.latex?\left&space;[&space;-1,1\right&space;]"  title="http://latex.codecogs.com/svg.latex?\left [ -1,1\right ]" />范围。                  
>
>或者采用均值归一化，<img  src="http://latex.codecogs.com/svg.latex?x_{i}=\frac{x_{i}-\mu_{i}}{s_{i}}"  title="http://latex.codecogs.com/svg.latex?x_{i}=\frac{x_{i}-\mu_{i}}{s_{i}}" />
>
>其中，<img  src="http://latex.codecogs.com/svg.latex?\mu_{i}"  title="http://latex.codecogs.com/svg.latex?\mu_{i}" />是X向量元素的平均值，<img src="http://latex.codecogs.com/svg.latex?s_{i}" title="http://latex.codecogs.com/svg.latex?s_{i}" />是最大值-最小值或者可以用标准偏差。
>
>通过特征缩放，能够使得训练过程更加有效的收敛。

# 2.2，构造特征以及多项式回归(Features and Polynomial Regression)

**（一）构造特征**

> 有些特征值不是特别好用，我们可以用这些特征值构造一个新特征

> 如房价预测问题，<img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)={\theta_{0}}+{\theta_{1}}\times{frontage}+{\theta_{2}}\times{depth}" title="http://latex.codecogs.com/svg.latex?h_{\theta}(x)={\theta_{0}}+{\theta_{1}}\times{frontage}+{\theta_{2}}\times{depth}" />
>
> <img src="http://latex.codecogs.com/svg.latex?{x_{1}}=frontage" title="http://latex.codecogs.com/svg.latex?{x_{1}}=frontage" />（临街宽度），
>
> <img src="http://latex.codecogs.com/svg.latex?{x_{2}}=depth" title="http://latex.codecogs.com/svg.latex?{x_{2}}=depth" />（纵向深度），
>
> <img src="http://latex.codecogs.com/svg.latex?x=frontage\times depth=area" title="http://latex.codecogs.com/svg.latexx=frontage\times depth=area" />（面积），
>
> 则：<img src="http://latex.codecogs.com/svg.latex?{h_{\theta}}(x)={\theta_{0}}+{\theta_{1}}x" title="http://latex.codecogs.com/svg.latex?{h_{\theta}}(x)={\theta_{0}}+{\theta_{1}}x" />。

（二），多项式回归

> 有时线性回归不适合于所有数据，用一组多项式来拟合出一条曲线来适应数据

> 线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，
>
> 比如一个二次方模型：<img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}" title="http://latex.codecogs.com/svg.latex?h_{\theta}(x)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}" />
>
> 或者三次方模型： <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}+{\theta_{3}}{x_{3}^3}" title="http://latex.codecogs.com/svg.latex?h_{\theta}(x)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}+{\theta_{3}}{x_{3}^3}" />
>
> 通常我们需要先观察数据然后再决定准备尝试怎样的模型。
>
> 另外，我们可以令：<img src="http://latex.codecogs.com/svg.latex?x_2=x{2}^{2},{{x}{3}}=x{3}^{3}" title="http://latex.codecogs.com/svg.latex?x_2=x{2}^{2},{{x}{3}}=x{3}^{3}" />，从而将模型转化为线性回归模型。
>
> 根据函数图形特性，我们还可以使：<img src="http://latex.codecogs.com/svg.latex?h_\theta(x)=\theta_0\text+\theta_1 (size)+\theta_2(size)^2" title="http://latex.codecogs.com/svg.latex?h_\theta(x)=\theta_0\text+\theta_1 (size)+\theta_2(size)^2" />
>
> 或者:<img src="http://latex.codecogs.com/svg.latex?h_\theta(x)=\theta_0\text+\theta_1(size)+\theta_2\sqrt size" title="http://latex.codecogs.com/svg.latex?h_\theta(x)=\theta_0\text+\theta_1(size)+\theta_2\sqrt size" />

> 注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。

# 3.3，正规方程(Normal Equation)

正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：<img src="http://latex.codecogs.com/svg.latex?\frac{\partial}{\partial{\theta_{j}}}J({\theta_{j}})=0" title="http://latex.codecogs.com/svg.latex?\frac{\partial}{\partial{\theta_{j}}}J({\theta_{j}})=0" />。

>假设我们的训练集特征矩阵为<img src="http://latex.codecogs.com/svg.latex?X" title="http://latex.codecogs.com/svg.latex?X" />（包含了<img src="http://latex.codecogs.com/svg.latex?{{x}_{0}}=1" title="http://latex.codecogs.com/svg.latex?{{x}_{0}}=1" />）并且我们的训练集结果为向量<img src="http://latex.codecogs.com/svg.latex?y" title="http://latex.codecogs.com/svg.latex?y" />，
>则利用正规方程解出向量 <img src="http://latex.codecogs.com/svg.latex?\theta ={{( {X^T}X)}^{-1}}{X^{T}}y" title="http://latex.codecogs.com/svg.latex?\theta ={{( {X^T}X)}^{-1}}{X^{T}}y" />。
>上标T代表矩阵转置，上标-1 代表矩阵的逆。



> 注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。

# 3.4，梯度下降与正规方程的比较

|             **梯度下降**             |                           正规方程                           |
| :----------------------------------: | :----------------------------------------------------------: |
| 需要选择合适的学习率(learning rate)α |                         不需要选择α                          |
|            需要很多轮迭代            |                     不需要迭代，一次搞定                     |
|    但是即使n很大的时候效果也很好     | 但是需要计算<img src="http://latex.codecogs.com/svg.latex?(X^TX)^{-1}" title="http://latex.codecogs.com/svg.latex?(X^TX)^{-1}" />，其时间复杂度是<img src="http://latex.codecogs.com/svg.latex?o(n^3)" title="http://latex.codecogs.com/svg.latex?o(n^3)" /> |

# 3.5，正规方程在矩阵不可逆情况下的解决方法

>1) 去掉冗余的特征（线性相关）：
>
>2) 过多的特征，删掉一些特征，或者使用regularization（正则化）–之后的课程会专门介绍。

